{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "from confusion_matrix import norm_confusion_metrics\n",
    "from confusion_matrix import get_confusion_metrics_score\n",
    "from confusion_matrix import accuracy, iou_conf, precision, recall\n",
    "\n",
    "\n",
    "conf_mat_dataframe = pd.read_csv('result/csvs/confusion_matrix.csv')\n",
    "\n",
    "labels = [\"Unrecognized\", \"Forest\", \"Builtup\", \"Water\", \"Farmland\", \"Meadow\"]\n",
    "conf_mat = []\n",
    "\n",
    "for col in range(8, conf_mat_dataframe.shape[1]):\n",
    "    col_name = conf_mat_dataframe.columns[col]\n",
    "    conf_mat_val = sum(conf_mat_dataframe[col_name])\n",
    "    conf_mat.append(conf_mat_val)\n",
    "\n",
    "conf_mat = np.reshape(conf_mat, (-1, 6))\n",
    "\n",
    "df_cm = pd.DataFrame(conf_mat, index = [i for i in labels], columns = [i for i in labels])\n",
    "plt.figure(num=None, figsize=(12,6), dpi=300)\n",
    "ax = sb.heatmap(df_cm, annot=True, linewidths=.3, cmap='Blues', fmt='d')\n",
    "plt.yticks(rotation=0)\n",
    "plt.xlabel('Predicted', size=12, color='g', labelpad=20)\n",
    "plt.ylabel('Actual', size=12, color='g')\n",
    "plt.title('Total Confusion Matrix', pad=15)\n",
    "ax.xaxis.tick_top() # x axis on top\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.savefig('result/csvs/total-conf-mat.png')\n",
    "\n",
    "df_cm.to_csv('result/csvs/total-conf-mat.csv', header=True)\n",
    "norm_conf = norm_confusion_metrics(conf_mat)\n",
    "\n",
    "df_cm = pd.DataFrame(norm_conf, index = [i for i in labels], columns = [i for i in labels])\n",
    "plt.figure(num=None, figsize=(12,6), dpi=300)\n",
    "ax = sb.heatmap(df_cm, annot=True, linewidths=.3, cmap='Blues', fmt='.4f')\n",
    "plt.yticks(rotation=0)\n",
    "plt.xlabel('Predicted', size=12, color='g', labelpad=20)\n",
    "plt.ylabel('Actual', size=12, color='g')\n",
    "plt.title('Normalize Confusion Matrx', pad=15)\n",
    "ax.xaxis.tick_top() # x axis on top\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.savefig('result/csvs/norm-total-conf-mat.png')\n",
    "\n",
    "output_metrics = np.zeros((5,7), dtype=np.object)\n",
    "output_metrics[0,:] = np.concatenate((['Metrics'], labels))\n",
    "output_metrics[1:,0] = ['Accuracy', 'IoU', 'Precision', 'Recall']\n",
    "\n",
    "mat = np.zeros((4,6))\n",
    "\n",
    "for i in range(6):\n",
    "    tp, fp, fn, tn = get_confusion_metrics_score(class_index=i, conf_metric=conf_mat)\n",
    "    \n",
    "    acc = accuracy(tp=tp, tn=tn, fp=fp, fn=fn)\n",
    "    iou_sc = iou_conf(tp=tp, fp=fp, fn=fn)\n",
    "    prec = precision(tp=tp, fp=fp)\n",
    "    rec = recall(tp=tp, fn=fn)\n",
    "    \n",
    "    mat[0,i] = acc\n",
    "    mat[1,i] = iou_sc\n",
    "    mat[2,i] = prec\n",
    "    mat[3,i] = rec\n",
    "\n",
    "    print(f'{labels[i]}')\n",
    "    print(f'\\tAccuracy: {acc}')\n",
    "    print(f'\\tIoU: {iou_sc}')\n",
    "    print(f'\\tPrecision: {prec}')\n",
    "    print(f'\\tRecall: {rec}')\n",
    "\n",
    "output_metrics[1:,1:] = mat\n",
    "output_mat = pd.DataFrame(output_metrics)\n",
    "output_mat.to_csv('result/csvs/overall-performance.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
